{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "bentoAICellStatus": "none",
    "bentoCellName": {
     "name": "Check GPU Status",
     "origin": "ai"
    },
    "executionStartTime": 1767292246810,
    "executionStopTime": 1767292249974,
    "isCommentPanelOpen": false,
    "language": "python",
    "originalKey": "b0d06cf5-6d80-48dc-8234-0eeb1b45ac91",
    "outputsInitialized": true,
    "requestMsgId": "8ff5d7dc-5034-46c0-8a54-d9ddf89586ad",
    "serverExecutionDuration": 540.37336999318
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "import torch\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "bentoAICellStatus": "none",
    "bentoCellName": {
     "name": "Display Local Changes",
     "origin": "ai"
    },
    "customInput": null,
    "executionStartTime": 1767292253764,
    "executionStopTime": 1767292266660,
    "isCommentPanelOpen": false,
    "language": "python",
    "originalKey": "5a7d2033-dc93-4aed-9dff-7b24eca3c773",
    "outputsInitialized": true,
    "requestMsgId": "25f5bc1d-41ad-436e-8763-6a1078655b04",
    "serverExecutionDuration": 12622.693403973,
    "showInput": true
   },
   "outputs": [],
   "source": [
    "%local-changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "bentoAICellStatus": "none",
    "bentoCellName": {
     "name": "Import Transfer Learning Modules",
     "origin": "ai"
    },
    "customInput": null,
    "executionStartTime": 1767292256008,
    "executionStopTime": 1767292266992,
    "isCommentPanelOpen": false,
    "language": "python",
    "originalKey": "f8debf90-ffa5-446a-aa78-3b1319add964",
    "outputsInitialized": true,
    "requestMsgId": "0b50e710-725a-4e24-b028-03282fc460f6",
    "serverExecutionDuration": 76.94331597304,
    "showInput": true
   },
   "outputs": [],
   "source": [
    "from mint.transfer_learning.engression_python import engression\n",
    "from mint.transfer_learning.quantile_match import quantile_matching_estimate\n",
    "from mint.transfer_learning.covariate_shift import kernel_mean_matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "bentoAICellStatus": "none",
    "customInput": null,
    "isCommentPanelOpen": false,
    "language": "markdown",
    "originalKey": "2bfbe94f-6645-44f9-831c-5b3eb7aa6234",
    "outputsInitialized": false,
    "showInput": false
   },
   "source": [
    "### Concept and Covariate Shifts (Multi-source)\n",
    "$P^{(0)}(x) \\neq P^{(k)}(x)$ and $P^{(0)}(y|x) \\neq P^{(k)}(y|x)$.\n",
    "\n",
    "Source domain: $Y^{(1)}= \\sin(3\\beta^TX) + 1 + \\epsilon$; \n",
    "and $Y^{(2)}= \\cos(3\\beta^TX) + 1 + \\epsilon$ with $X\\sim N(\\mathbf{1}_d, I_d)$.\n",
    "\n",
    "Target domain: $Y= \\sin(3\\beta^TX)/3 - 3 + \\epsilon$ with $X\\sim N(\\mathbf{0}_d, 0.5^2 I_d)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "bentoAICellStatus": "none",
    "bentoCellName": {
     "name": "Generate synthetic datasets",
     "origin": "ai"
    },
    "customInput": null,
    "executionStartTime": 1767292271857,
    "executionStopTime": 1767292272501,
    "isCommentPanelOpen": false,
    "language": "python",
    "originalKey": "96e49295-1a1a-438c-b5a7-92333fd50712",
    "outputsInitialized": true,
    "requestMsgId": "497048b8-ad69-43c9-a837-c76a47a50d04",
    "serverExecutionDuration": 3.7639189977199,
    "showInput": true
   },
   "outputs": [],
   "source": [
    "# d = 5\n",
    "# n_s = 1000\n",
    "# # Source data\n",
    "# np.random.rand(123)\n",
    "# Sigma = np.eye(d)\n",
    "# mu1 = np.ones(d)\n",
    "# sig = 0.5\n",
    "# X_dat1 = np.random.multivariate_normal(mean=mu1, cov=Sigma, size=n_s)\n",
    "# beta1 = 1 / np.arange(1, d+1)\n",
    "# Y1 = np.sin(3*np.dot(X_dat1, beta1)) + 1 + np.random.randn(n_s)*sig\n",
    "\n",
    "# X_dat2 = np.random.multivariate_normal(mean=mu1, cov=Sigma, size=n_s)\n",
    "# Y2 = 2*np.cos(3*np.dot(X_dat2, beta1)) + 1 + np.random.randn(n_s)*sig\n",
    "\n",
    "# # Target data\n",
    "# n0 = 50\n",
    "# mu0 = np.zeros(d)\n",
    "# X_dat0 = np.random.multivariate_normal(mean=mu0, cov=0.25*Sigma, size=n0)\n",
    "# Y0 = np.sin(3*np.dot(X_dat0, beta1))/3 - 1 + np.random.randn(n0)*sig\n",
    "\n",
    "# X_dat0_full = np.random.multivariate_normal(mean=mu0, cov=0.25*Sigma, size=2*n_s+n0)\n",
    "# Y0_full = np.sin(3*np.dot(X_dat0_full, beta1))/3 - 1 + np.random.randn(2*n_s+n0)*sig\n",
    "\n",
    "# X_test0 = np.random.multivariate_normal(mean=mu0, cov=0.25*Sigma, size=5000)\n",
    "# Y0_test = np.sin(3*np.dot(X_test0, beta1))/3 - 1 + np.random.randn(5000)*sig\n",
    "\n",
    "def sim_data(n_s=1000, n_0=50, n_test=5000, d=5, sig=0.5, mu_s=np.ones(5), mu_t=np.zeros(5), Sigma=np.eye(5), beta1=1/np.arange(1, 6)):\n",
    "    # Target data\n",
    "    X_dat0 = np.random.multivariate_normal(mean=mu_t, cov=0.25*Sigma, size=n_0)\n",
    "    Y0 = np.sin(3*np.dot(X_dat0, beta1))/3 - 1 + np.random.randn(n_0)*sig\n",
    "    dat0 = np.column_stack([Y0, X_dat0])\n",
    "    \n",
    "    # Source data\n",
    "    X_dat1 = np.random.multivariate_normal(mean=mu_s, cov=Sigma, size=n_s)\n",
    "    Y1 = np.sin(3*np.dot(X_dat1, beta1)) + 1 + np.random.randn(n_s)*sig\n",
    "    dat1 = np.column_stack([Y1, X_dat1])\n",
    "\n",
    "    X_dat2 = np.random.multivariate_normal(mean=mu_s, cov=Sigma, size=n_s)\n",
    "    Y2 = 2*np.cos(3*np.dot(X_dat2, beta1)) + 1 + np.random.randn(n_s)*sig\n",
    "    dat2 = np.column_stack([Y2, X_dat2])\n",
    "\n",
    "    dat_source = [dat1, dat2]\n",
    "\n",
    "    X_dat0_full = np.random.multivariate_normal(mean=mu_t, cov=0.25*Sigma, size=2*n_s+n_0)\n",
    "    Y0_full = np.sin(3*np.dot(X_dat0_full, beta1))/3 - 1 + np.random.randn(2*n_s+n_0)*sig\n",
    "    dat0_full = np.column_stack([Y0_full, X_dat0_full])\n",
    "\n",
    "    X_test0 = np.random.multivariate_normal(mean=mu_t, cov=0.25*Sigma, size=n_test)\n",
    "    Y0_test = np.sin(3*np.dot(X_test0, beta1))/3 - 1 + np.random.randn(n_test)*sig\n",
    "    dat_test0 = np.column_stack([Y0_test, X_test0])\n",
    "\n",
    "    return dat_source, dat0, dat0_full, dat_test0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "bentoCellName": {
     "name": "Change working directory",
     "origin": "ai"
    },
    "customInput": null,
    "executionStartTime": 1767292292976,
    "executionStopTime": 1767292293625,
    "language": "python",
    "originalKey": "b9e4471a-f443-42a6-9436-36431e04fd1f",
    "outputsInitialized": true,
    "requestMsgId": "37b53e15-171e-4fbd-9407-46b088b630cb",
    "serverExecutionDuration": 2.954815980047,
    "showInput": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/data/sandcastle/boxes/fbsource/fbcode/mint/transfer_learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "bentoAICellStatus": "none",
    "customInput": null,
    "isCommentPanelOpen": false,
    "language": "markdown",
    "originalKey": "19d602a7-89f1-4b3a-8a03-04e87b9fdb4c",
    "outputsInitialized": false,
    "showInput": false
   },
   "source": [
    "## Target-only ML regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "bentoAICellStatus": "none",
    "bentoCellName": {
     "name": "Simulate datasets",
     "origin": "ai"
    },
    "customInput": null,
    "executionStartTime": 1767144530688,
    "executionStopTime": null,
    "isCommentPanelOpen": false,
    "language": "python",
    "originalKey": "ee0a18b3-3ead-4205-b115-572cf003b017",
    "outputsInitialized": true,
    "requestMsgId": "5a907c9a-ccf4-40e8-b418-834d4db8241e",
    "serverExecutionDuration": null,
    "showInput": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/data/sandcastle/boxes/fbsource/fbcode/mint/transfer_learning\")\n",
    "\n",
    "B = 100\n",
    "for n_s in [500, 1000, 2000, 5000]:\n",
    "    res_full = pd.DataFrame()\n",
    "    xbg_mse = np.zeros(B)\n",
    "    krr_mse = np.zeros(B)\n",
    "    nn_mse = np.zeros(B)\n",
    "    for b in range(B):\n",
    "        d = 5\n",
    "        np.random.seed(b)\n",
    "        dat_source, dat0, dat0_full, dat_test = sim_data(n_s=n_s, n_0=50, n_test=2000, d=d, sig=0.5, mu_s=np.ones(d), mu_t=np.zeros(d), Sigma=np.eye(d), beta1=1/np.arange(1, d+1))\n",
    "\n",
    "        # Target-only ML models\n",
    "        X0 = dat0[:, 1:]\n",
    "        Y0 = dat0[:, 0]\n",
    "        X_test = dat_test[:, 1:]\n",
    "        Y_test = dat_test[:, 0]\n",
    "\n",
    "        ## XGBoost\n",
    "        param_grid = {\n",
    "            'learning_rate': [0.001, 0.01, 0.1],\n",
    "            'n_estimators': [10, 50, 100], \n",
    "            'max_depth': [3, 5],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0],\n",
    "        }\n",
    "        xgb_model = XGBRegressor(objective='reg:squarederror', random_state=0)\n",
    "        grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "        grid_search.fit(X0, Y0)\n",
    "        target_only_xgb = grid_search.best_estimator_\n",
    "        xbg_mse[b] = np.mean(abs(target_only_xgb.predict(X_test) - Y_test)**2)\n",
    "\n",
    "        ## Kernel Ridge Regression\n",
    "        alpha_lst = (0.1 / X0.shape[0] * (3.0 ** np.array(range(-2,6))))\n",
    "        param_grid = {'alpha': alpha_lst}\n",
    "        target_only_krr = KernelRidge(kernel='rbf')\n",
    "        grid_search = GridSearchCV(target_only_krr, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "        grid_search.fit(X0, Y0)\n",
    "        target_only_krr = grid_search.best_estimator_\n",
    "        krr_mse[b] = np.mean(abs(target_only_krr.predict(X_test) - Y_test)**2)\n",
    "\n",
    "        ## Neural Network\n",
    "        param_grid = {\n",
    "            'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
    "            'alpha': [0.0001, 0.001, 0.01],\n",
    "        }\n",
    "        mlp = MLPRegressor(max_iter=200, random_state=0)\n",
    "        grid_search = GridSearchCV(mlp, param_grid, cv=5)\n",
    "        grid_search.fit(X0, Y0)\n",
    "        target_only_mlp = grid_search.best_estimator_\n",
    "        nn_mse[b] = np.mean(abs(target_only_mlp.predict(X_test) - Y_test)**2)\n",
    "\n",
    "    target_only_mse = np.concatenate([xbg_mse, krr_mse, nn_mse], axis=0)\n",
    "    res = pd.DataFrame(target_only_mse, columns=['MSE'])\n",
    "    res['Method'] = np.repeat(['Target-only XGBoost', 'Target-only Kernel Ridge Regression', 'Target-only Neural Network'], B)\n",
    "    res['Sample_size'] = [n_s] * 3 * B\n",
    "    res_full = pd.concat([res_full, res], axis=0)\n",
    "    res_full.to_csv('Results/Simulation_Concept_Covariate'+str(n_s)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "bentoAICellStatus": "none",
    "customInput": null,
    "isCommentPanelOpen": false,
    "language": "markdown",
    "originalKey": "0fbe5963-39a8-4400-9625-2f9cc924c33b",
    "outputsInitialized": false,
    "showInput": false
   },
   "source": [
    "## Oracle ML Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "bentoAICellStatus": "none",
    "bentoCellName": {
     "name": "Evaluate Model Performance",
     "origin": "ai"
    },
    "customInput": null,
    "executionStartTime": 1767144542522,
    "executionStopTime": null,
    "isCommentPanelOpen": false,
    "language": "python",
    "originalKey": "9c9d27a0-021e-46ba-9ece-8e46cc90d6e7",
    "outputsInitialized": true,
    "requestMsgId": "42667c8c-d898-4fbf-9f9a-3a7f4f047c39",
    "serverExecutionDuration": null,
    "showInput": true
   },
   "outputs": [],
   "source": [
    "B = 100\n",
    "for n_s in [500, 1000, 2000, 5000]:\n",
    "    res_full = pd.read_csv('Results/Simulation_Concept_Covariate'+str(n_s)+'.csv')\n",
    "    xbg_mse = np.zeros(B)\n",
    "    krr_mse = np.zeros(B)\n",
    "    nn_mse = np.zeros(B)\n",
    "    for b in range(B):\n",
    "        d = 5\n",
    "        np.random.seed(b)\n",
    "        dat_source, dat0, dat0_full, dat_test = sim_data(n_s=n_s, n_0=50, n_test=5000, d=d, sig=0.5, mu_s=np.ones(d), mu_t=np.zeros(d), Sigma=np.eye(d), beta1=1/np.arange(1, d+1))\n",
    "\n",
    "        # Oracle ML models\n",
    "        X0_full = dat0_full[:, 1:]\n",
    "        Y0_full = dat0_full[:, 0]\n",
    "        X_test = dat_test[:, 1:]\n",
    "        Y_test = dat_test[:, 0]\n",
    "\n",
    "        ## XGBoost\n",
    "        param_grid = {\n",
    "            'learning_rate': [0.001, 0.01, 0.1],\n",
    "            'n_estimators': [10, 50, 100], \n",
    "            'max_depth': [3, 5],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0],\n",
    "        }\n",
    "        xgb_model = XGBRegressor(objective='reg:squarederror', random_state=0)\n",
    "        grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "        grid_search.fit(X0_full, Y0_full)\n",
    "        target_only_xgb = grid_search.best_estimator_\n",
    "        xbg_mse[b] = np.mean(abs(target_only_xgb.predict(X_test) - Y_test)**2)\n",
    "\n",
    "        ## Kernel Ridge Regression\n",
    "        alpha_lst = (0.1 / X0_full.shape[0] * (3.0 ** np.array(range(-2,6))))\n",
    "        param_grid = {'alpha': alpha_lst}\n",
    "        target_only_krr = KernelRidge(kernel='rbf')\n",
    "        grid_search = GridSearchCV(target_only_krr, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "        grid_search.fit(X0_full, Y0_full)\n",
    "        target_only_krr = grid_search.best_estimator_\n",
    "        krr_mse[b] = np.mean(abs(target_only_krr.predict(X_test) - Y_test)**2)\n",
    "\n",
    "        ## Neural Network\n",
    "        param_grid = {\n",
    "            'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
    "            'alpha': [0.0001, 0.001, 0.01],\n",
    "        }\n",
    "        mlp = MLPRegressor(max_iter=200, random_state=0)\n",
    "        grid_search = GridSearchCV(mlp, param_grid, cv=5)\n",
    "        grid_search.fit(X0_full, Y0_full)\n",
    "        target_only_mlp = grid_search.best_estimator_\n",
    "        nn_mse[b] = np.mean(abs(target_only_mlp.predict(X_test) - Y_test)**2)\n",
    "\n",
    "    oracle_mse = np.concatenate([xbg_mse, krr_mse, nn_mse])\n",
    "    res = pd.DataFrame(oracle_mse, columns=['MSE'])\n",
    "    res['Method'] = np.repeat(['Oracle XGBoost', 'Oracle Kernel Ridge Regression', 'Oracle Neural Network'], B)\n",
    "    res_full = pd.concat([res_full, res], axis=0)\n",
    "    res_full.to_csv('Results/Simulation_Concept_Covariate'+str(n_s)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "bentoAICellStatus": "none",
    "bentoCellName": {
     "name": "Calculate Oracle MSE",
     "origin": "ai"
    },
    "customInput": null,
    "executionStartTime": 1767128839658,
    "executionStopTime": 1767129308667,
    "isCommentPanelOpen": false,
    "language": "python",
    "originalKey": "365c3277-44f7-434a-a607-a26f69885029",
    "output": {
     "id": "889254366997333",
     "output_revision_id": "886334220617600"
    },
    "outputsInitialized": true,
    "requestMsgId": "22753bf7-6d1b-4eb8-8ebc-98af5e5c6dcb",
    "serverExecutionDuration": 4.0260860114358,
    "showInput": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27121768, 0.26467678, 0.25556036],\n",
       "       [0.26973405, 0.26496096, 0.25455988],\n",
       "       [0.27139313, 0.27023601, 0.25679947]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracle_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "bentoCellName": {
     "name": "Cell 12",
     "origin": "initial"
    },
    "customInput": null,
    "executionStartTime": 1767126558579,
    "executionStopTime": 1767126558873,
    "language": "python",
    "originalKey": "30442bc8-c50f-4510-861f-41129dfe41df",
    "outputsInitialized": true,
    "requestMsgId": "485ca8f2-9b86-42e6-8bbb-4cd0ea1e91de",
    "serverExecutionDuration": 0,
    "showInput": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "bentoAICellStatus": "none",
    "customInput": null,
    "isCommentPanelOpen": false,
    "language": "markdown",
    "originalKey": "d6811acc-0029-475f-89e4-ddf2c51c2773",
    "outputsInitialized": false,
    "showInput": false
   },
   "source": [
    "### ML Regression with Conditional Quantile Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "bentoAICellStatus": "none",
    "bentoCellName": {
     "name": "Evaluate Model with Simulated Data",
     "origin": "ai"
    },
    "customInput": null,
    "executionStartTime": 1767292304968,
    "executionStopTime": null,
    "isCommentPanelOpen": false,
    "language": "python",
    "originalKey": "d544b18b-9632-4586-a823-8def551bfc4e",
    "output": {
     "id": "1122595769790728",
     "output_revision_id": "867446292542452"
    },
    "outputsInitialized": true,
    "requestMsgId": "b2c14039-ada8-4107-ba46-8fc6185c308e",
    "serverExecutionDuration": null,
    "showInput": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.6976,  E(|Y-Yhat|): 0.8664,  E(|Yhat-Yhat'|): 0.3376\n",
      "[Epoch 100 (10%)] energy-loss: 0.6106,  E(|Y-Yhat|): 0.9160,  E(|Yhat-Yhat'|): 0.6107\n",
      "[Epoch 200 (20%)] energy-loss: 0.5978,  E(|Y-Yhat|): 1.0119,  E(|Yhat-Yhat'|): 0.8283\n",
      "[Epoch 300 (30%)] energy-loss: 0.5838,  E(|Y-Yhat|): 1.0759,  E(|Yhat-Yhat'|): 0.9841\n",
      "[Epoch 400 (40%)] energy-loss: 0.5861,  E(|Y-Yhat|): 1.1561,  E(|Yhat-Yhat'|): 1.1401\n",
      "[Epoch 500 (50%)] energy-loss: 0.5807,  E(|Y-Yhat|): 1.1506,  E(|Yhat-Yhat'|): 1.1398\n",
      "[Epoch 600 (60%)] energy-loss: 0.5616,  E(|Y-Yhat|): 1.1547,  E(|Yhat-Yhat'|): 1.1862\n",
      "[Epoch 700 (70%)] energy-loss: 0.5711,  E(|Y-Yhat|): 1.1489,  E(|Yhat-Yhat'|): 1.1556\n",
      "[Epoch 800 (80%)] energy-loss: 0.5611,  E(|Y-Yhat|): 1.1438,  E(|Yhat-Yhat'|): 1.1655\n",
      "[Epoch 900 (90%)] energy-loss: 0.5584,  E(|Y-Yhat|): 1.1553,  E(|Yhat-Yhat'|): 1.1940\n",
      "[Epoch 1000 (100%)] energy-loss: 0.5820,  E(|Y-Yhat|): 1.1722,  E(|Yhat-Yhat'|): 1.1805\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.5049,  E(|Y-Yhat|): 1.0241,  E(|Yhat-Yhat'|): 1.0383\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.7732,  E(|Y-Yhat|): 0.9715,  E(|Yhat-Yhat'|): 0.3964\n",
      "[Epoch 100 (10%)] energy-loss: 0.6337,  E(|Y-Yhat|): 0.9447,  E(|Yhat-Yhat'|): 0.6221\n",
      "[Epoch 200 (20%)] energy-loss: 0.5961,  E(|Y-Yhat|): 1.0341,  E(|Yhat-Yhat'|): 0.8760\n",
      "[Epoch 300 (30%)] energy-loss: 0.5822,  E(|Y-Yhat|): 1.0991,  E(|Yhat-Yhat'|): 1.0338\n",
      "[Epoch 400 (40%)] energy-loss: 0.5726,  E(|Y-Yhat|): 1.1351,  E(|Yhat-Yhat'|): 1.1249\n",
      "[Epoch 500 (50%)] energy-loss: 0.5705,  E(|Y-Yhat|): 1.1708,  E(|Yhat-Yhat'|): 1.2006\n",
      "[Epoch 600 (60%)] energy-loss: 0.5853,  E(|Y-Yhat|): 1.2039,  E(|Yhat-Yhat'|): 1.2372\n",
      "[Epoch 700 (70%)] energy-loss: 0.5872,  E(|Y-Yhat|): 1.1970,  E(|Yhat-Yhat'|): 1.2196\n",
      "[Epoch 800 (80%)] energy-loss: 0.5809,  E(|Y-Yhat|): 1.1967,  E(|Yhat-Yhat'|): 1.2315\n",
      "[Epoch 900 (90%)] energy-loss: 0.5700,  E(|Y-Yhat|): 1.2048,  E(|Yhat-Yhat'|): 1.2696\n",
      "[Epoch 1000 (100%)] energy-loss: 0.5750,  E(|Y-Yhat|): 1.1875,  E(|Yhat-Yhat'|): 1.2250\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9302,  E(|Y-Yhat|): 1.8635,  E(|Yhat-Yhat'|): 1.8665\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.7176,  E(|Y-Yhat|): 0.9024,  E(|Yhat-Yhat'|): 0.3695\n",
      "[Epoch 100 (10%)] energy-loss: 0.6398,  E(|Y-Yhat|): 0.9256,  E(|Yhat-Yhat'|): 0.5716\n",
      "[Epoch 200 (20%)] energy-loss: 0.5920,  E(|Y-Yhat|): 1.0166,  E(|Yhat-Yhat'|): 0.8490\n",
      "[Epoch 300 (30%)] energy-loss: 0.5658,  E(|Y-Yhat|): 1.0726,  E(|Yhat-Yhat'|): 1.0135\n",
      "[Epoch 400 (40%)] energy-loss: 0.5686,  E(|Y-Yhat|): 1.1291,  E(|Yhat-Yhat'|): 1.1210\n",
      "[Epoch 500 (50%)] energy-loss: 0.5530,  E(|Y-Yhat|): 1.1389,  E(|Yhat-Yhat'|): 1.1717\n",
      "[Epoch 600 (60%)] energy-loss: 0.5809,  E(|Y-Yhat|): 1.1753,  E(|Yhat-Yhat'|): 1.1888\n",
      "[Epoch 700 (70%)] energy-loss: 0.5921,  E(|Y-Yhat|): 1.1862,  E(|Yhat-Yhat'|): 1.1881\n",
      "[Epoch 800 (80%)] energy-loss: 0.5433,  E(|Y-Yhat|): 1.1331,  E(|Yhat-Yhat'|): 1.1797\n",
      "[Epoch 900 (90%)] energy-loss: 0.5427,  E(|Y-Yhat|): 1.1574,  E(|Yhat-Yhat'|): 1.2294\n",
      "[Epoch 1000 (100%)] energy-loss: 0.5696,  E(|Y-Yhat|): 1.1702,  E(|Yhat-Yhat'|): 1.2012\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.5155,  E(|Y-Yhat|): 1.0161,  E(|Yhat-Yhat'|): 1.0013\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.7277,  E(|Y-Yhat|): 0.9116,  E(|Yhat-Yhat'|): 0.3677\n",
      "[Epoch 100 (10%)] energy-loss: 0.6469,  E(|Y-Yhat|): 0.9655,  E(|Yhat-Yhat'|): 0.6372\n",
      "[Epoch 200 (20%)] energy-loss: 0.6196,  E(|Y-Yhat|): 1.0595,  E(|Yhat-Yhat'|): 0.8798\n",
      "[Epoch 300 (30%)] energy-loss: 0.5948,  E(|Y-Yhat|): 1.1360,  E(|Yhat-Yhat'|): 1.0824\n",
      "[Epoch 400 (40%)] energy-loss: 0.5944,  E(|Y-Yhat|): 1.1704,  E(|Yhat-Yhat'|): 1.1520\n",
      "[Epoch 500 (50%)] energy-loss: 0.5866,  E(|Y-Yhat|): 1.1960,  E(|Yhat-Yhat'|): 1.2188\n",
      "[Epoch 600 (60%)] energy-loss: 0.6077,  E(|Y-Yhat|): 1.2287,  E(|Yhat-Yhat'|): 1.2421\n",
      "[Epoch 700 (70%)] energy-loss: 0.5780,  E(|Y-Yhat|): 1.2069,  E(|Yhat-Yhat'|): 1.2579\n",
      "[Epoch 800 (80%)] energy-loss: 0.5897,  E(|Y-Yhat|): 1.1982,  E(|Yhat-Yhat'|): 1.2171\n",
      "[Epoch 900 (90%)] energy-loss: 0.5710,  E(|Y-Yhat|): 1.1827,  E(|Yhat-Yhat'|): 1.2234\n",
      "[Epoch 1000 (100%)] energy-loss: 0.5731,  E(|Y-Yhat|): 1.1855,  E(|Yhat-Yhat'|): 1.2248\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.8941,  E(|Y-Yhat|): 1.8282,  E(|Yhat-Yhat'|): 1.8682\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.7108,  E(|Y-Yhat|): 0.8894,  E(|Yhat-Yhat'|): 0.3572\n",
      "[Epoch 100 (10%)] energy-loss: 0.6309,  E(|Y-Yhat|): 0.9312,  E(|Yhat-Yhat'|): 0.6005\n",
      "[Epoch 200 (20%)] energy-loss: 0.5746,  E(|Y-Yhat|): 0.9986,  E(|Yhat-Yhat'|): 0.8480\n",
      "[Epoch 300 (30%)] energy-loss: 0.5620,  E(|Y-Yhat|): 1.0648,  E(|Yhat-Yhat'|): 1.0057\n",
      "[Epoch 400 (40%)] energy-loss: 0.5933,  E(|Y-Yhat|): 1.1117,  E(|Yhat-Yhat'|): 1.0368\n",
      "[Epoch 500 (50%)] energy-loss: 0.5601,  E(|Y-Yhat|): 1.1272,  E(|Yhat-Yhat'|): 1.1341\n",
      "[Epoch 600 (60%)] energy-loss: 0.5631,  E(|Y-Yhat|): 1.1448,  E(|Yhat-Yhat'|): 1.1634\n",
      "[Epoch 700 (70%)] energy-loss: 0.5678,  E(|Y-Yhat|): 1.1629,  E(|Yhat-Yhat'|): 1.1900\n",
      "[Epoch 800 (80%)] energy-loss: 0.5669,  E(|Y-Yhat|): 1.1625,  E(|Yhat-Yhat'|): 1.1913\n",
      "[Epoch 900 (90%)] energy-loss: 0.5985,  E(|Y-Yhat|): 1.1839,  E(|Yhat-Yhat'|): 1.1708\n",
      "[Epoch 1000 (100%)] energy-loss: 0.6027,  E(|Y-Yhat|): 1.1761,  E(|Yhat-Yhat'|): 1.1468\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.5019,  E(|Y-Yhat|): 1.0073,  E(|Yhat-Yhat'|): 1.0108\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.7301,  E(|Y-Yhat|): 0.8884,  E(|Yhat-Yhat'|): 0.3165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 100 (10%)] energy-loss: 0.6495,  E(|Y-Yhat|): 0.9408,  E(|Yhat-Yhat'|): 0.5826\n",
      "[Epoch 200 (20%)] energy-loss: 0.6018,  E(|Y-Yhat|): 1.0338,  E(|Yhat-Yhat'|): 0.8641\n",
      "[Epoch 300 (30%)] energy-loss: 0.6032,  E(|Y-Yhat|): 1.1164,  E(|Yhat-Yhat'|): 1.0264\n",
      "[Epoch 400 (40%)] energy-loss: 0.5911,  E(|Y-Yhat|): 1.1802,  E(|Yhat-Yhat'|): 1.1781\n",
      "[Epoch 500 (50%)] energy-loss: 0.5863,  E(|Y-Yhat|): 1.1854,  E(|Yhat-Yhat'|): 1.1982\n",
      "[Epoch 600 (60%)] energy-loss: 0.5338,  E(|Y-Yhat|): 1.1715,  E(|Yhat-Yhat'|): 1.2755\n",
      "[Epoch 700 (70%)] energy-loss: 0.5647,  E(|Y-Yhat|): 1.1958,  E(|Yhat-Yhat'|): 1.2622\n",
      "[Epoch 800 (80%)] energy-loss: 0.5635,  E(|Y-Yhat|): 1.1795,  E(|Yhat-Yhat'|): 1.2320\n",
      "[Epoch 900 (90%)] energy-loss: 0.5527,  E(|Y-Yhat|): 1.2039,  E(|Yhat-Yhat'|): 1.3023\n",
      "[Epoch 1000 (100%)] energy-loss: 0.5851,  E(|Y-Yhat|): 1.2055,  E(|Yhat-Yhat'|): 1.2409\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.8370,  E(|Y-Yhat|): 1.7603,  E(|Yhat-Yhat'|): 1.8466\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.7221,  E(|Y-Yhat|): 0.8982,  E(|Yhat-Yhat'|): 0.3521\n",
      "[Epoch 100 (10%)] energy-loss: 0.6460,  E(|Y-Yhat|): 0.9372,  E(|Yhat-Yhat'|): 0.5824\n",
      "[Epoch 200 (20%)] energy-loss: 0.5982,  E(|Y-Yhat|): 1.0020,  E(|Yhat-Yhat'|): 0.8078\n",
      "[Epoch 300 (30%)] energy-loss: 0.5727,  E(|Y-Yhat|): 1.0936,  E(|Yhat-Yhat'|): 1.0418\n",
      "[Epoch 400 (40%)] energy-loss: 0.5902,  E(|Y-Yhat|): 1.1252,  E(|Yhat-Yhat'|): 1.0700\n",
      "[Epoch 500 (50%)] energy-loss: 0.5737,  E(|Y-Yhat|): 1.1806,  E(|Yhat-Yhat'|): 1.2137\n",
      "[Epoch 600 (60%)] energy-loss: 0.5667,  E(|Y-Yhat|): 1.1591,  E(|Yhat-Yhat'|): 1.1848\n",
      "[Epoch 700 (70%)] energy-loss: 0.5918,  E(|Y-Yhat|): 1.2072,  E(|Yhat-Yhat'|): 1.2308\n",
      "[Epoch 800 (80%)] energy-loss: 0.5831,  E(|Y-Yhat|): 1.1919,  E(|Yhat-Yhat'|): 1.2175\n",
      "[Epoch 900 (90%)] energy-loss: 0.5790,  E(|Y-Yhat|): 1.1904,  E(|Yhat-Yhat'|): 1.2227\n",
      "[Epoch 1000 (100%)] energy-loss: 0.5683,  E(|Y-Yhat|): 1.1544,  E(|Yhat-Yhat'|): 1.1722\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4964,  E(|Y-Yhat|): 1.0117,  E(|Yhat-Yhat'|): 1.0307\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.7474,  E(|Y-Yhat|): 0.9317,  E(|Yhat-Yhat'|): 0.3686\n",
      "[Epoch 100 (10%)] energy-loss: 0.6304,  E(|Y-Yhat|): 0.9434,  E(|Yhat-Yhat'|): 0.6261\n",
      "[Epoch 200 (20%)] energy-loss: 0.6040,  E(|Y-Yhat|): 1.0376,  E(|Yhat-Yhat'|): 0.8672\n",
      "[Epoch 300 (30%)] energy-loss: 0.5918,  E(|Y-Yhat|): 1.1145,  E(|Yhat-Yhat'|): 1.0454\n",
      "[Epoch 400 (40%)] energy-loss: 0.5653,  E(|Y-Yhat|): 1.1350,  E(|Yhat-Yhat'|): 1.1394\n",
      "[Epoch 500 (50%)] energy-loss: 0.5898,  E(|Y-Yhat|): 1.1635,  E(|Yhat-Yhat'|): 1.1473\n",
      "[Epoch 600 (60%)] energy-loss: 0.5700,  E(|Y-Yhat|): 1.1843,  E(|Yhat-Yhat'|): 1.2286\n",
      "[Epoch 700 (70%)] energy-loss: 0.5827,  E(|Y-Yhat|): 1.2063,  E(|Yhat-Yhat'|): 1.2473\n",
      "[Epoch 800 (80%)] energy-loss: 0.5776,  E(|Y-Yhat|): 1.2184,  E(|Yhat-Yhat'|): 1.2815\n",
      "[Epoch 900 (90%)] energy-loss: 0.6231,  E(|Y-Yhat|): 1.2405,  E(|Yhat-Yhat'|): 1.2349\n",
      "[Epoch 1000 (100%)] energy-loss: 0.5708,  E(|Y-Yhat|): 1.2170,  E(|Yhat-Yhat'|): 1.2924\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.8682,  E(|Y-Yhat|): 1.8114,  E(|Yhat-Yhat'|): 1.8864\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.6888,  E(|Y-Yhat|): 0.8884,  E(|Yhat-Yhat'|): 0.3991\n",
      "[Epoch 100 (10%)] energy-loss: 0.5956,  E(|Y-Yhat|): 0.9410,  E(|Yhat-Yhat'|): 0.6907\n",
      "[Epoch 200 (20%)] energy-loss: 0.5840,  E(|Y-Yhat|): 1.0530,  E(|Yhat-Yhat'|): 0.9380\n",
      "[Epoch 300 (30%)] energy-loss: 0.5802,  E(|Y-Yhat|): 1.1101,  E(|Yhat-Yhat'|): 1.0599\n",
      "[Epoch 400 (40%)] energy-loss: 0.5473,  E(|Y-Yhat|): 1.1233,  E(|Yhat-Yhat'|): 1.1521\n",
      "[Epoch 500 (50%)] energy-loss: 0.5424,  E(|Y-Yhat|): 1.1237,  E(|Yhat-Yhat'|): 1.1626\n",
      "[Epoch 600 (60%)] energy-loss: 0.5576,  E(|Y-Yhat|): 1.1537,  E(|Yhat-Yhat'|): 1.1923\n",
      "[Epoch 700 (70%)] energy-loss: 0.5912,  E(|Y-Yhat|): 1.1766,  E(|Yhat-Yhat'|): 1.1709\n",
      "[Epoch 800 (80%)] energy-loss: 0.5804,  E(|Y-Yhat|): 1.1542,  E(|Yhat-Yhat'|): 1.1475\n",
      "[Epoch 900 (90%)] energy-loss: 0.5816,  E(|Y-Yhat|): 1.1636,  E(|Yhat-Yhat'|): 1.1641\n",
      "[Epoch 1000 (100%)] energy-loss: 0.5877,  E(|Y-Yhat|): 1.1668,  E(|Yhat-Yhat'|): 1.1582\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.4996,  E(|Y-Yhat|): 1.0207,  E(|Yhat-Yhat'|): 1.0421\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Batch is larger than half of the sample size. Training based on full-batch gradient descent.\n",
      "[Epoch 1 (0%)] energy-loss: 0.6998,  E(|Y-Yhat|): 0.9082,  E(|Yhat-Yhat'|): 0.4168\n",
      "[Epoch 100 (10%)] energy-loss: 0.6502,  E(|Y-Yhat|): 0.9982,  E(|Yhat-Yhat'|): 0.6959\n",
      "[Epoch 200 (20%)] energy-loss: 0.5923,  E(|Y-Yhat|): 1.0552,  E(|Yhat-Yhat'|): 0.9257\n",
      "[Epoch 300 (30%)] energy-loss: 0.5733,  E(|Y-Yhat|): 1.1187,  E(|Yhat-Yhat'|): 1.0908\n",
      "[Epoch 400 (40%)] energy-loss: 0.5647,  E(|Y-Yhat|): 1.1492,  E(|Yhat-Yhat'|): 1.1691\n",
      "[Epoch 500 (50%)] energy-loss: 0.5661,  E(|Y-Yhat|): 1.1817,  E(|Yhat-Yhat'|): 1.2311\n",
      "[Epoch 600 (60%)] energy-loss: 0.5510,  E(|Y-Yhat|): 1.2046,  E(|Yhat-Yhat'|): 1.3073\n",
      "[Epoch 700 (70%)] energy-loss: 0.5859,  E(|Y-Yhat|): 1.1907,  E(|Yhat-Yhat'|): 1.2096\n",
      "[Epoch 800 (80%)] energy-loss: 0.5710,  E(|Y-Yhat|): 1.2204,  E(|Yhat-Yhat'|): 1.2987\n",
      "[Epoch 900 (90%)] energy-loss: 0.5583,  E(|Y-Yhat|): 1.1895,  E(|Yhat-Yhat'|): 1.2623\n",
      "[Epoch 1000 (100%)] energy-loss: 0.6044,  E(|Y-Yhat|): 1.2200,  E(|Yhat-Yhat'|): 1.2313\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.8647,  E(|Y-Yhat|): 1.8244,  E(|Yhat-Yhat'|): 1.9194\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    }
   ],
   "source": [
    "B = 100\n",
    "for n_s in [500, 1000, 2000, 5000]:\n",
    "    res_full = pd.read_csv('Results/Simulation_Concept_Covariate'+str(n_s)+'.csv')\n",
    "    xbg_mse = np.zeros(B)\n",
    "    krr_mse = np.zeros(B)\n",
    "    nn_mse = np.zeros(B)\n",
    "    for b in range(B):\n",
    "        d = 5\n",
    "        np.random.seed(b)\n",
    "        dat_source, dat0, dat0_full, dat_test = sim_data(n_s=1000, n_0=50, n_test=5000, d=d, sig=0.5, mu_s=np.ones(d), mu_t=np.zeros(d), Sigma=np.eye(d), beta1=1/np.arange(1, d+1))\n",
    "\n",
    "        # ML models with transfer learning with conditional quantile matching\n",
    "        X_dat0 = dat0[:, 1:]\n",
    "        Y0 = dat0[:, 0]\n",
    "        X_test = dat_test[:, 1:]\n",
    "        Y_test = dat_test[:, 0]\n",
    "\n",
    "        # Fit the engression generative model on each source data\n",
    "        eng_mod = []\n",
    "        X_source_tensor = []\n",
    "        for i in range(len(dat_source)):\n",
    "            Y_tensor = torch.tensor(dat_source[i][:, 0].reshape(-1,1), dtype=torch.float32)\n",
    "            X_tensor = torch.tensor(dat_source[i][:, 1:], dtype=torch.float32)\n",
    "            engressor = engression(X_tensor, Y_tensor, num_layer=2, hidden_dim=100, noise_dim=100, lr=0.0001, num_epochs=1000)\n",
    "            X_source_tensor.append(X_tensor)\n",
    "            eng_mod.append(engressor)\n",
    "        X_dat0_tensor = torch.tensor(X_dat0, dtype=torch.float32)\n",
    "        X_source_tensor = torch.cat(X_source_tensor, dim=0)\n",
    "\n",
    "        # Sample response variables from each source data based on the covariates in the target domain\n",
    "        N_sam = 3000\n",
    "        Y0_sam = []\n",
    "        for i in range(len(eng_mod)):\n",
    "            Y0_sam.append(eng_mod[i].sample(X_dat0_tensor, sample_size=N_sam).detach().numpy().reshape(-1,1))\n",
    "        Y0_sam = np.concatenate(Y0_sam, axis=1)\n",
    "        Y0_sam_arr = np.concatenate([np.ones([Y0_sam.shape[0],1]), Y0_sam], axis=1)\n",
    "\n",
    "        beta_sol = quantile_matching_estimate(np.repeat(Y0, N_sam), Y0_sam_arr, beta_init=None, stop_eps=1e-8, max_iter=1000, verbose=False)\n",
    "\n",
    "        Y_source_pred = []\n",
    "        for i in range(len(eng_mod)):\n",
    "            Y_source_pred.append(eng_mod[i].predict(X_source_tensor, sample_size=100).detach().numpy().reshape(-1,1))\n",
    "        Y_source_pred = np.concatenate(Y_source_pred, axis=1)\n",
    "        Y_source_pred = np.concatenate([np.ones([Y_source_pred.shape[0],1]), Y_source_pred], axis=1)\n",
    "        Y_matched = np.dot(Y_source_pred, beta_sol)\n",
    "\n",
    "        # Kernel mean matching for covariate shift correction\n",
    "        X_source = X_source_tensor.detach().numpy()\n",
    "        kmm_weights = kernel_mean_matching(X_test, X_source, kern='rbf', B=10)[:,0]\n",
    "\n",
    "        X_comb = np.concatenate([X_source, X_dat0], axis=0)\n",
    "        Y_comb = np.concatenate([Y_matched, Y0], axis=0)\n",
    "        weights = np.concatenate([kmm_weights, np.ones(X_dat0.shape[0])], axis=0)\n",
    "        ## XGBoost\n",
    "        param_grid = {\n",
    "            'learning_rate': [0.001, 0.01, 0.1],\n",
    "            'n_estimators': [10, 50, 100], \n",
    "            'max_depth': [3, 5],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0],\n",
    "        }\n",
    "        xgb_model = XGBRegressor(objective='reg:squarederror', random_state=0)\n",
    "        grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "        grid_search.fit(X_comb, Y_comb, sample_weight=weights)\n",
    "        target_only_xgb = grid_search.best_estimator_\n",
    "        xbg_mse[b] = np.mean(abs(target_only_xgb.predict(X_test) - Y_test)**2)\n",
    "\n",
    "        ## Kernel Ridge Regression\n",
    "        alpha_lst = (0.1 / X_comb.shape[0] * (3.0 ** np.array(range(-2,6))))\n",
    "        param_grid = {'alpha': alpha_lst}\n",
    "        target_only_krr = KernelRidge(kernel='rbf')\n",
    "        grid_search = GridSearchCV(target_only_krr, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "        grid_search.fit(X_comb, Y_comb, sample_weight=weights)\n",
    "        target_only_krr = grid_search.best_estimator_\n",
    "        krr_mse[b] = np.mean(abs(target_only_krr.predict(X_test) - Y_test)**2)\n",
    "\n",
    "        ## Neural Network\n",
    "        param_grid = {\n",
    "            'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
    "            'alpha': [0.0001, 0.001, 0.01],\n",
    "        }\n",
    "        mlp = MLPRegressor(max_iter=200, random_state=0)\n",
    "        grid_search = GridSearchCV(mlp, param_grid, cv=5)\n",
    "        grid_search.fit(X_comb, Y_comb)\n",
    "        target_only_mlp = grid_search.best_estimator_\n",
    "        nn_mse[b] = np.mean(abs(target_only_mlp.predict(X_test) - Y_test)**2)\n",
    "\n",
    "    tlcqm_mse = np.concatenate([xbg_mse, krr_mse, nn_mse])\n",
    "    res = pd.DataFrame(oracle_mse, columns=['MSE'])\n",
    "    res['Method'] = np.repeat(['TLCQM XGBoost', 'TLCQM Kernel Ridge Regression', 'TLCQM Neural Network'], B)\n",
    "    res_full = pd.concat([res_full, res], axis=0)\n",
    "    res_full.to_csv('Results/Simulation_Concept_Covariate'+str(n_s)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "bentoCellName": {
     "name": "Calculate MSE",
     "origin": "ai"
    },
    "customInput": null,
    "executionStartTime": 1767128846855,
    "executionStopTime": 1767129631599,
    "language": "python",
    "originalKey": "a9d76c32-6ff3-4e40-8a97-4ff33d6ccf4b",
    "output": {
     "id": "819188537807831",
     "output_revision_id": "2115917419266852"
    },
    "outputsInitialized": true,
    "requestMsgId": "68036a57-2800-4465-8fbc-4d53693abb8c",
    "serverExecutionDuration": 5.1427470170893,
    "showInput": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.31767674, 0.31296973, 0.31640316],\n",
       "       [0.32529719, 0.32822047, 0.33123258],\n",
       "       [0.31225785, 0.30412624, 0.31737129]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tlcqm_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "bentoCellName": {
     "name": "Cell 16",
     "origin": "initial"
    },
    "customInput": null,
    "language": "python",
    "originalKey": "0ab7115b-c048-4f54-a30c-d611dd20a16a",
    "showInput": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "customInput": null,
    "language": "markdown",
    "originalKey": "939ad090-a7f2-4f77-97d0-4c6c2a826e34",
    "showInput": false
   },
   "source": [
    "## Pseudo-labeling Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "bentoCellName": {
     "name": "Import KRR Covariate Shift",
     "origin": "ai"
    },
    "customInput": null,
    "executionStartTime": 1767133015804,
    "executionStopTime": 1767133016434,
    "language": "python",
    "originalKey": "7473db54-067f-4e8a-92bb-cd8c452eb838",
    "outputsInitialized": true,
    "requestMsgId": "a04ffb42-a438-42bc-beec-6307ca1a84a4",
    "serverExecutionDuration": 2.1577549632639,
    "showInput": true
   },
   "outputs": [],
   "source": [
    "# from mint.transfer_learning.KRR_pseudo_label import KRR_covariate_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "bentoCellName": {
     "name": "Simulate data and initialize KRR",
     "origin": "ai"
    },
    "customInput": null,
    "executionStartTime": 1767133008948,
    "executionStopTime": 1767133009643,
    "language": "python",
    "originalKey": "73699663-6b2a-4e13-9ec7-0c580e04df83",
    "outputsInitialized": true,
    "requestMsgId": "da13ca7a-212b-44e4-9520-6964f045b78a",
    "serverExecutionDuration": 3.1173129682429,
    "showInput": true
   },
   "outputs": [],
   "source": [
    "# B = 3\n",
    "# pskrr_mse = np.zeros(B)\n",
    "# for b in range(B):\n",
    "#     d = 5\n",
    "#     np.random.seed(b)\n",
    "#     dat_source, dat0, dat0_full, dat_test = sim_data(n_s=1000, n_0=50, n_test=5000, d=d, sig=0.5, mu_s=np.ones(d), mu_t=np.zeros(d), Sigma=np.eye(d), beta1=1/np.arange(1, d+1))\n",
    "\n",
    "#     # ML models with pseudo-labeling with KRR\n",
    "#     X_test = dat_test[:, 1:]\n",
    "#     Y_test = dat_test[:, 0]\n",
    "\n",
    "#     X_source = np.concatenate([dat_source[0][:, 1:], dat_source[1][:, 1:]], axis=0)\n",
    "#     Y_source = np.concatenate([dat_source[0][:, 0], dat_source[1][:, 0]], axis=0)\n",
    "#     print(X_source.shape)\n",
    "#     krrPL = KRR_covariate_shift(n = X_source.shape[0], n_0 = X_test.shape[0], B = 1, sigma = 1, X = X_source, y = Y_source, \n",
    "#                                 X_0=X_test, y_0=np.ones([X_test.shape[0],]), seed=0)\n",
    "#     krrPL.fit(rho = 0.1, beta = 2)\n",
    "#     Y_pred = krrPL.predict_final(X_test)\n",
    "#     pskrr_mse[b] = np.mean(abs(Y_pred - Y_test)**2)\n",
    "#     print(pskrr_mse[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "bentoCellName": {
     "name": "Cell 20",
     "origin": "initial"
    },
    "customInput": null,
    "executionStartTime": 1767129830719,
    "executionStopTime": 1767130408169,
    "language": "python",
    "originalKey": "55298a5b-98a1-4474-8c09-2c409554658a",
    "requestMsgId": "be6f6458-467b-4c0a-9677-3708be81f1da",
    "serverExecutionDuration": null,
    "showInput": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "bentoCellName": {
     "name": "Cell 21",
     "origin": "initial"
    },
    "customInput": null,
    "language": "python",
    "originalKey": "156d59a4-6d64-41d7-8165-45d85a35f1f8",
    "showInput": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "customInput": null,
    "language": "markdown",
    "originalKey": "6b96fbfe-5f4e-4794-a03b-12d80b08c3b0",
    "showInput": false
   },
   "source": [
    "## Transfer Learning with Kernel Ridge Regression (TKRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "bentoCellName": {
     "name": "Generate Simulated Data",
     "origin": "ai"
    },
    "customInput": null,
    "executionStartTime": 1767132045734,
    "executionStopTime": 1767132065644,
    "language": "python",
    "originalKey": "08677f99-6f20-420e-abf6-14cf594760f0",
    "outputsInitialized": true,
    "requestMsgId": "ea68a802-aa19-4d08-bddd-7327151b78e7",
    "serverExecutionDuration": 19596.912720008,
    "showInput": true
   },
   "outputs": [],
   "source": [
    "# B = 3\n",
    "# for n_s in [500, 1000, 2000, 5000]:\n",
    "#     tkrr_mse = np.zeros(B)\n",
    "#     for b in range(B):\n",
    "#         d = 5\n",
    "#         np.random.seed(b)\n",
    "#         dat_source, dat0, dat0_full, dat_test = sim_data(n_s=1000, n_0=50, n_test=5000, d=d, sig=0.5, mu_s=np.ones(d), mu_t=np.zeros(d), Sigma=np.eye(d), beta1=1/np.arange(1, d+1))\n",
    "#         # Prepare data\n",
    "#         X_source = [dat[:, 1:] for dat in dat_source]\n",
    "#         Y_source = [dat[:, 0] for dat in dat_source]\n",
    "#         X0_train = dat0[:, 1:]\n",
    "#         Y0_train = dat0[:, 0]\n",
    "#         X0_test = dat_test[:, 1:]\n",
    "#         Y0_test = dat_test[:, 0]\n",
    "\n",
    "#         # Train KRR on combined data\n",
    "#         X_comb = np.concatenate(X_source + [X0_train], axis=0)\n",
    "#         Y_comb = np.concatenate(Y_source + [Y0_train], axis=0)\n",
    "#         alpha_lst = 0.1 / X_comb.shape[0] * (3.0 ** np.arange(-3, 7))\n",
    "#         param_grid = {'alpha': alpha_lst}\n",
    "#         comb_krr = KernelRidge(kernel=\"rbf\")\n",
    "#         grid_search = GridSearchCV(comb_krr, param_grid, cv=5, scoring=\"neg_mean_squared_error\")\n",
    "#         grid_search.fit(X_comb, Y_comb)\n",
    "#         comb_krr = grid_search.best_estimator_\n",
    "#         # Fit residual model\n",
    "#         Y0_pred = comb_krr.predict(X0_train)\n",
    "#         Y_resi = Y0_train - Y0_pred\n",
    "#         alpha_lst = 0.1 / X0_train.shape[0] * (3.0 ** np.arange(-3, 7))\n",
    "#         param_grid = {\"alpha\": alpha_lst}\n",
    "#         resi_krr = KernelRidge(kernel=\"rbf\")\n",
    "#         grid_search = GridSearchCV(resi_krr, param_grid, cv=5, scoring=\"neg_mean_squared_error\")\n",
    "#         grid_search.fit(X0_train, Y_resi)\n",
    "#         resi_krr = grid_search.best_estimator_\n",
    "#         # Predict on test data\n",
    "#         Y0_pred_new = resi_krr.predict(X0_test) + comb_krr.predict(X0_test)\n",
    "#         tkrr_mse[b] = np.mean(abs(Y0_pred_new - Y0_test) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "bentoCellName": {
     "name": "Compute MSE",
     "origin": "ai"
    },
    "customInput": null,
    "executionStartTime": 1767132909810,
    "executionStopTime": 1767132941769,
    "language": "python",
    "originalKey": "4e009a10-8b0d-4a91-922b-dbb5f6d837af",
    "output": {
     "id": "2100640630765207",
     "output_revision_id": "2356027371506346"
    },
    "outputsInitialized": true,
    "requestMsgId": "da296e69-2062-4a1a-90bb-a771ce2ea5d3",
    "serverExecutionDuration": 31604.886885034,
    "showInput": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SA-TKRR MSEs over B runs: [0.34925179 0.31861234 0.36970078]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import combinations\n",
    "def fit_krr(X, Y, alpha_grid=None):\n",
    "    if alpha_grid is None:\n",
    "        alpha_grid = 0.1 / X.shape[0] * (3.0 ** np.arange(-3, 7))\n",
    "    param_grid = {'alpha': alpha_grid}\n",
    "    krr = KernelRidge(kernel=\"rbf\")\n",
    "    grid_search = GridSearchCV(krr, param_grid, cv=5, scoring=\"neg_mean_squared_error\")\n",
    "    grid_search.fit(X, Y)\n",
    "    return grid_search.best_estimator_\n",
    "def rkhs_norm(f1, f2, X):\n",
    "    # Use L2 norm of predictions as a proxy for RKHS norm\n",
    "    return np.linalg.norm(f1.predict(X) - f2.predict(X))\n",
    "\n",
    "\n",
    "B = 3\n",
    "for n_s in [500, 1000, 2000, 5000]:\n",
    "    tkrr_mse = np.zeros(B)\n",
    "    for b in range(B):\n",
    "        d = 5\n",
    "        np.random.seed(b)\n",
    "        dat_source, dat0, dat0_full, dat_test = sim_data(n_s=1000, n_0=50, n_test=5000, d=d, sig=0.5, mu_s=np.ones(d), mu_t=np.zeros(d), Sigma=np.eye(d), beta1=1/np.arange(1, d+1))\n",
    "        # Prepare data\n",
    "        X_source = [dat[:, 1:] for dat in dat_source]\n",
    "        Y_source = [dat[:, 0] for dat in dat_source]\n",
    "        X0 = dat0[:, 1:]\n",
    "        Y0 = dat0[:, 0]\n",
    "        X0_test = dat_test[:, 1:]\n",
    "        Y0_test = dat_test[:, 0]\n",
    "        m = len(X_source)\n",
    "        # --- Step 1: Split target data into T1 and T2 ---\n",
    "        X0_T1, X0_T2, Y0_T1, Y0_T2 = train_test_split(X0, Y0, test_size=0.5, random_state=b)\n",
    "        # Further split T2 into T21 and T22 for aggregation\n",
    "        X0_T21, X0_T22, Y0_T21, Y0_T22 = train_test_split(X0_T2, Y0_T2, test_size=0.5, random_state=b)\n",
    "        # --- Step 2: Fit KRR on each source and T1 ---\n",
    "        fb0 = fit_krr(X0_T1, Y0_T1)  # Target model on T1\n",
    "        fbk_list = [fit_krr(Xk, Yk) for Xk, Yk in zip(X_source, Y_source)]  # Source models\n",
    "        # --- Step 3: Compute contrast functions and RKHS norms ---\n",
    "        norms = [rkhs_norm(fbk, fb0, X0_T1) for fbk in fbk_list]\n",
    "        ranks = np.argsort(norms)  # Indices of sources sorted by similarity\n",
    "        # --- Step 4: Build candidate models using increasing numbers of sources ---\n",
    "        candidate_models = [fb0]  # fb0 corresponds to Ab0 = \n",
    "        for ell in range(1, m+1):\n",
    "            selected_indices = ranks[:ell]\n",
    "            # Combine selected sources and T1\n",
    "            X_comb = np.concatenate([X_source[i] for i in selected_indices] + [X0_T1], axis=0)\n",
    "            Y_comb = np.concatenate([Y_source[i] for i in selected_indices] + [Y0_T1], axis=0)\n",
    "            # Transferring step\n",
    "            comb_krr = fit_krr(X_comb, Y_comb)\n",
    "            # Debiasing step\n",
    "            Y0_pred = comb_krr.predict(X0_T1)\n",
    "            Y_resi = Y0_T1 - Y0_pred\n",
    "            resi_krr = fit_krr(X0_T1, Y_resi)\n",
    "            # Final model: sum of transferring and debiasing\n",
    "            class CombinedModel:\n",
    "                def __init__(self, m1, m2):\n",
    "                    self.m1 = m1\n",
    "                    self.m2 = m2\n",
    "                def predict(self, X):\n",
    "                    return self.m1.predict(X) + self.m2.predict(X)\n",
    "            fb_ell = CombinedModel(comb_krr, resi_krr)\n",
    "            candidate_models.append(fb_ell)\n",
    "        # --- Step 5: Hyper-sparse aggregation (convex combination of at most two models) ---\n",
    "        # Evaluate risk on T21 for each candidate\n",
    "        risks = [np.mean((model.predict(X0_T21) - Y0_T21)**2) for model in candidate_models]\n",
    "        best_idx = np.argmin(risks)\n",
    "        # Find best convex combination of two models\n",
    "        min_risk = risks[best_idx]\n",
    "        best_combo = (best_idx, None, 1.0)\n",
    "        for i, j in combinations(range(len(candidate_models)), 2):\n",
    "            # Find optimal t in [0,1] for convex combination\n",
    "            preds_i = candidate_models[i].predict(X0_T21)\n",
    "            preds_j = candidate_models[j].predict(X0_T21)\n",
    "            t_vals = np.linspace(0, 1, 101)\n",
    "            for t in t_vals:\n",
    "                preds = t * preds_i + (1-t) * preds_j\n",
    "                risk = np.mean((preds - Y0_T21)**2)\n",
    "                if risk < min_risk:\n",
    "                    min_risk = risk\n",
    "                    best_combo = (i, j, t)\n",
    "        # Build final aggregated model\n",
    "        i, j, t = best_combo\n",
    "        class AggregatedModel:\n",
    "            def __init__(self, m1, m2, t):\n",
    "                self.m1 = m1\n",
    "                self.m2 = m2\n",
    "                self.t = t\n",
    "            def predict(self, X):\n",
    "                if self.m2 is None:\n",
    "                    return self.m1.predict(X)\n",
    "                return self.t * self.m1.predict(X) + (1-self.t) * self.m2.predict(X)\n",
    "        if j is not None:\n",
    "            fba = AggregatedModel(candidate_models[i], candidate_models[j], t)\n",
    "        else:\n",
    "            fba = AggregatedModel(candidate_models[i], None, t)\n",
    "        # --- Step 6: Evaluate on test set ---\n",
    "        Y0_pred_new = fba.predict(X0_test)\n",
    "        tkrr_mse[b] = np.mean((Y0_pred_new - Y0_test) ** 2)\n",
    "# print(\"SA-TKRR MSEs over B runs:\", tkrr_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "bentoCellName": {
     "name": "Calculate MSE",
     "origin": "ai"
    },
    "customInput": null,
    "executionStartTime": 1767132481232,
    "executionStopTime": 1767132481568,
    "language": "python",
    "originalKey": "84009223-f883-4e1a-81e9-912abfb86423",
    "output": {
     "id": "2086921145454752",
     "output_revision_id": "1218286509636317"
    },
    "outputsInitialized": true,
    "requestMsgId": "ede14fd7-9ea0-4d29-8c21-f92642e27c86",
    "serverExecutionDuration": 4.023349029012,
    "showInput": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.34141852, 0.34105561, 0.32420118])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tkrr_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "bentoCellName": {
     "name": "Cell 26",
     "origin": "initial"
    },
    "customInput": null,
    "language": "python",
    "originalKey": "b17192eb-7582-4873-abe1-fd8acb2021cf",
    "showInput": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "customInput": null,
    "language": "markdown",
    "originalKey": "2a185a38-7beb-4a46-b208-978a2d34389b",
    "showInput": false
   },
   "source": [
    "## Deep Transfer Learning for Conditional Shift in Regression (CDAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "bentoCellName": {
     "name": "Define TargetCNN Model",
     "origin": "ai"
    },
    "customInput": null,
    "executionStartTime": 1767133606752,
    "executionStopTime": 1767133606924,
    "language": "python",
    "originalKey": "e96be5b4-80c1-435b-a214-65c63257256d",
    "outputsInitialized": true,
    "requestMsgId": "a492cf01-cfb8-4531-a9d7-a96825cbdb75",
    "serverExecutionDuration": 3.1833740067668,
    "showInput": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example TargetCNN definition for d=5\n",
    "class TargetCNN(nn.Module):\n",
    "    def __init__(self, d=5):\n",
    "        super(TargetCNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(d, 384)\n",
    "        self.fc2 = nn.Linear(384, 64)\n",
    "        self.fc3 = nn.Linear(64, 16)\n",
    "        self.predict = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        inter_x1 = self.relu(x)\n",
    "        x = self.fc2(inter_x1)\n",
    "        inter_x2 = self.relu(x)\n",
    "        x = self.fc3(inter_x2)\n",
    "        inter_x3 = self.relu(x)\n",
    "        result = self.predict(inter_x3)\n",
    "        target_list = [inter_x1, inter_x2, inter_x3]\n",
    "        return target_list, result\n",
    "\n",
    "def to_tensor(X, Y):\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    Y_tensor = torch.tensor(Y, dtype=torch.float32).reshape(-1, 1)\n",
    "    return X_tensor, Y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "bentoCellName": {
     "name": "Define Custom Kernels",
     "origin": "ai"
    },
    "customInput": null,
    "executionStartTime": 1767133750974,
    "executionStopTime": 1767133751349,
    "language": "python",
    "originalKey": "75cff9c9-cc57-4b94-a6e8-f473436e3f97",
    "outputsInitialized": true,
    "requestMsgId": "365d01b4-c1e9-4212-b8b9-cddadb12c686",
    "serverExecutionDuration": 3.966587013565,
    "showInput": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def rbf_kernel(X, Y, gamma=0.4):\n",
    "    # X: (n_samples_X, n_features)\n",
    "    # Y: (n_samples_Y, n_features)\n",
    "    X = X if X.ndim == 2 else X.view(X.size(0), -1)\n",
    "    Y = Y if Y.ndim == 2 else Y.view(Y.size(0), -1)\n",
    "    XX = torch.sum(X ** 2, 1).view(-1, 1)\n",
    "    YY = torch.sum(Y ** 2, 1).view(1, -1)\n",
    "    distances = XX + YY - 2 * torch.mm(X, Y.t())\n",
    "    K = torch.exp(-gamma * distances)\n",
    "    return K\n",
    "\n",
    "def MLcon_kernel(source_list, source_pred, target_list, target_y, lamda=1.0):\n",
    "    # Use only the first layer's features for simplicity\n",
    "    X_p = source_list[0]  # (n_source, n_features)\n",
    "    Y_p = source_pred     # (n_source, 1)\n",
    "    X_q = target_list[0]  # (n_target, n_features)\n",
    "    Y_q = target_y        # (n_target, 1)\n",
    "\n",
    "    np_ = X_p.shape[0]\n",
    "    nq_ = X_q.shape[0]\n",
    "    I1 = torch.eye(np_, device=X_p.device)\n",
    "    I2 = torch.eye(nq_, device=X_q.device)\n",
    "\n",
    "    Kxpxp = rbf_kernel(X_p, X_p)\n",
    "    Kxqxq = rbf_kernel(X_q, X_q)\n",
    "    Kxqxp = rbf_kernel(X_q, X_p)\n",
    "    Kypyq = rbf_kernel(Y_p, Y_q)\n",
    "    Kyqyq = rbf_kernel(Y_q, Y_q)\n",
    "    Kypyp = rbf_kernel(Y_p, Y_p)\n",
    "\n",
    "    a = torch.mm(torch.inverse(Kxpxp + np_ * lamda * I1), Kypyp)\n",
    "    b = torch.mm(a, torch.inverse(Kxpxp + np_ * lamda * I1))\n",
    "    c = torch.mm(b, Kxpxp)\n",
    "    out1 = torch.trace(c)\n",
    "\n",
    "    a1 = torch.mm(torch.inverse(Kxqxq + nq_ * lamda * I2), Kyqyq)\n",
    "    b1 = torch.mm(a1, torch.inverse(Kxqxq + nq_ * lamda * I2))\n",
    "    c1 = torch.mm(b1, Kxqxq)\n",
    "    out2 = torch.trace(c1)\n",
    "\n",
    "    a2 = torch.mm(torch.inverse(Kxpxp + np_ * lamda * I1), Kypyq)\n",
    "    b2 = torch.mm(a2, torch.inverse(Kxqxq + nq_ * lamda * I2))\n",
    "    c2 = torch.mm(b2, Kxqxp)\n",
    "    out3 = torch.trace(c2)\n",
    "\n",
    "    out = out1 + out2 - 2 * out3\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "bentoCellName": {
     "name": "Simulate and Prepare Data",
     "origin": "ai"
    },
    "customInput": null,
    "executionStartTime": 1767133805381,
    "executionStopTime": 1767134026622,
    "language": "python",
    "originalKey": "9767e1eb-7816-48d2-ae0f-f4b0c872aa3c",
    "output": {
     "id": "1561684611532631",
     "output_revision_id": "26230854016517218"
    },
    "outputsInitialized": true,
    "requestMsgId": "3688db67-7650-40d8-ac21-b3f3010716ee",
    "serverExecutionDuration": 221044.35178195,
    "showInput": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, Epoch 0, Loss: 1.6087405681610107\n",
      "Fold 0, Epoch 100, Loss: 0.16194851696491241\n",
      "Fold 0, Test MSE: 0.4690\n",
      "Fold 1, Epoch 0, Loss: 1.2023255825042725\n",
      "Fold 1, Epoch 100, Loss: 0.16687558591365814\n",
      "Fold 1, Test MSE: 0.4984\n",
      "Fold 2, Epoch 0, Loss: 1.7064645290374756\n",
      "Fold 2, Epoch 100, Loss: 0.20977719128131866\n",
      "Fold 2, Test MSE: 0.6742\n"
     ]
    }
   ],
   "source": [
    "B = 3\n",
    "cdar_mse = np.zeros(B)\n",
    "\n",
    "for b in range(B):\n",
    "    d = 5\n",
    "    np.random.seed(b)\n",
    "    dat_source, dat0, dat0_full, dat_test = sim_data(\n",
    "        n_s=1000, n_0=50, n_test=5000, d=d, sig=0.5,\n",
    "        mu_s=np.ones(d), mu_t=np.zeros(d), Sigma=np.eye(d), beta1=1/np.arange(1, d+1)\n",
    "    )\n",
    "    # Prepare data\n",
    "    X_source = [dat[:, 1:] for dat in dat_source]\n",
    "    Y_source = [dat[:, 0] for dat in dat_source]\n",
    "    X0_train = dat0[:, 1:]\n",
    "    Y0_train = dat0[:, 0]\n",
    "    X0_test = dat_test[:, 1:]\n",
    "    Y0_test = dat_test[:, 0]\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_source_tensor, Y_source_tensor = to_tensor(X_source[0], Y_source[0])  # Use first source domain\n",
    "    X0_train_tensor, Y0_train_tensor = to_tensor(X0_train, Y0_train)\n",
    "    X0_test_tensor, Y0_test_tensor = to_tensor(X0_test, Y0_test)\n",
    "\n",
    "    # Initialize model\n",
    "    model = TargetCNN(d=d)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # CDAR hyperparameters\n",
    "    Lambda = 1.0\n",
    "    Beta = 1.0\n",
    "    num_epochs = 200\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        # Forward pass for source and target\n",
    "        source_list, source_pred = model(X_source_tensor)\n",
    "        target_list, target_pred = model(X0_train_tensor)\n",
    "\n",
    "        # Compute CEOD loss (replace with your actual implementation)\n",
    "        CEOD_loss = MLcon_kernel(\n",
    "        source_list, source_pred, target_list, Y0_train_tensor\n",
    "        )\n",
    "\n",
    "        # Hybrid loss\n",
    "        loss = Lambda * criterion(target_pred, Y0_train_tensor) + Beta * CEOD_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Fold {b}, Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, y_pred = model(X0_test_tensor)\n",
    "        y_pred = y_pred.numpy().flatten()\n",
    "        mse = np.mean((Y0_test - y_pred) ** 2)\n",
    "        cdar_mse[b] = mse\n",
    "        print(f\"Fold {b}, Test MSE: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "bentoCellName": {
     "name": "Cell 31",
     "origin": "initial"
    },
    "customInput": null,
    "language": "python",
    "originalKey": "716b1f88-9da2-464f-b407-aa09bcca5772",
    "showInput": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "customInput": null,
    "language": "markdown",
    "originalKey": "29bbcd67-fc15-4bb2-bd02-d3ff59e46d5c",
    "showInput": false
   },
   "source": [
    "## Multi-Domain Adaptation for Regression under Conditional shift (DARC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "bentoCellName": {
     "name": "Define FeatureExtractor Class",
     "origin": "ai"
    },
    "customInput": null,
    "executionStartTime": 1767136047063,
    "executionStopTime": 1767136047368,
    "language": "python",
    "originalKey": "87780678-e3ac-451a-8e70-017de0a9d4f4",
    "outputsInitialized": true,
    "requestMsgId": "83517eb5-6d64-4bf0-b922-7edc86ea733e",
    "serverExecutionDuration": 5.8365109725855,
    "showInput": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim, domain_dim, feature_dim=2):\n",
    "        super().__init__()\n",
    "        self.domain_embed = nn.Embedding(domain_dim, 8)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim + 8, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, feature_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, domain_id):\n",
    "        domain_vec = self.domain_embed(domain_id)\n",
    "        x_cat = torch.cat([x, domain_vec], dim=1)\n",
    "        return self.mlp(x_cat)\n",
    "\n",
    "def psp_loss(features, labels):\n",
    "    dist_matrix = torch.cdist(features, features, p=2)\n",
    "    label_matrix = torch.abs(labels.unsqueeze(0) - labels.unsqueeze(1))\n",
    "    return nn.functional.mse_loss(dist_matrix, label_matrix)\n",
    "\n",
    "class LinearRegressor(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(feature_dim, 1)\n",
    "\n",
    "    def forward(self, features):\n",
    "        return self.linear(features).squeeze(-1)\n",
    "\n",
    "def train_feature_extractor(F, X, Y, domain_ids, epochs=20, lr=1e-3):\n",
    "    F = F.to(device)\n",
    "    optimizer = optim.Adam(F.parameters(), lr=lr)\n",
    "    X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    Y = torch.tensor(Y, dtype=torch.float32).to(device)\n",
    "    domain_ids = torch.tensor(domain_ids, dtype=torch.long).to(device)\n",
    "    for epoch in range(epochs):\n",
    "        F.train()\n",
    "        features = F(X, domain_ids)\n",
    "        loss = psp_loss(features, Y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}, PSP Loss: {loss.item():.4f}\")\n",
    "    return F\n",
    "\n",
    "def train_regressor(F, R, X, Y, domain_ids, epochs=10, lr=1e-3):\n",
    "    F = F.to(device)\n",
    "    R = R.to(device)\n",
    "    optimizer = optim.Adam(R.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "    Y = torch.tensor(Y, dtype=torch.float32).to(device)\n",
    "    domain_ids = torch.tensor(domain_ids, dtype=torch.long).to(device)\n",
    "    for epoch in range(epochs):\n",
    "        F.eval()\n",
    "        with torch.no_grad():\n",
    "            features = F(X, domain_ids)\n",
    "        preds = R(features)\n",
    "        loss = loss_fn(preds, Y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Regressor Loss: {loss.item():.4f}\")\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "bentoCellName": {
     "name": "Simulate and Stack Data",
     "origin": "ai"
    },
    "customInput": null,
    "executionStartTime": 1767136048943,
    "executionStopTime": 1767136049682,
    "language": "python",
    "originalKey": "e86155f7-5c13-4100-a67f-2406ff313f5a",
    "output": {
     "id": "2071780490235311",
     "output_revision_id": "1227057562612880"
    },
    "outputsInitialized": true,
    "requestMsgId": "e59e4a20-dd03-448e-8009-881741ed7af5",
    "serverExecutionDuration": 560.0426569581,
    "showInput": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, PSP Loss: 2.4587\n",
      "Epoch 10, PSP Loss: 1.9449\n",
      "Epoch 15, PSP Loss: 1.6350\n",
      "Epoch 20, PSP Loss: 1.5797\n",
      "Epoch 5, Regressor Loss: 2.7535\n",
      "Epoch 10, Regressor Loss: 2.7392\n",
      "Fold 1, Test MSE: 0.9207\n",
      "Epoch 5, PSP Loss: 2.2825\n",
      "Epoch 10, PSP Loss: 1.8244\n",
      "Epoch 15, PSP Loss: 1.6174\n",
      "Epoch 20, PSP Loss: 1.5314\n",
      "Epoch 5, Regressor Loss: 5.6395\n",
      "Epoch 10, Regressor Loss: 5.5880\n",
      "Fold 2, Test MSE: 2.1952\n",
      "Epoch 5, PSP Loss: 2.2956\n",
      "Epoch 10, PSP Loss: 1.8233\n",
      "Epoch 15, PSP Loss: 1.5867\n",
      "Epoch 20, PSP Loss: 1.5437\n",
      "Epoch 5, Regressor Loss: 2.4045\n",
      "Epoch 10, Regressor Loss: 2.3868\n",
      "Fold 3, Test MSE: 1.7316\n"
     ]
    }
   ],
   "source": [
    "B = 3\n",
    "darc_mse = np.zeros(B)\n",
    "d = 5\n",
    "\n",
    "for b in range(B):\n",
    "    np.random.seed(b)\n",
    "    dat_source, dat0, dat0_full, dat_test = sim_data(\n",
    "        n_s=1000, n_0=50, n_test=5000, d=d, sig=0.5,\n",
    "        mu_s=np.ones(d), mu_t=np.zeros(d), Sigma=np.eye(d), beta1=1/np.arange(1, d+1)\n",
    "    )\n",
    "    # Multi-source domains\n",
    "    num_source_domains = len(dat_source)\n",
    "    # Stack all source domains\n",
    "    X_source = np.vstack([dat[:, 1:] for dat in dat_source])\n",
    "    Y_source = np.concatenate([dat[:, 0] for dat in dat_source])\n",
    "    domain_ids_source = np.concatenate([\n",
    "        np.full(len(dat_source[i]), i, dtype=int) for i in range(num_source_domains)\n",
    "    ])\n",
    "    # Target domain\n",
    "    X_target = dat0[:, 1:]\n",
    "    Y_target = dat0[:, 0]\n",
    "    domain_ids_target = np.full(len(X_target), num_source_domains, dtype=int)  # Target domain ID\n",
    "    # Combine for training\n",
    "    X_train = np.vstack([X_source, X_target])\n",
    "    Y_train = np.concatenate([Y_source, Y_target])\n",
    "    domain_ids_train = np.concatenate([domain_ids_source, domain_ids_target])\n",
    "    # Train DARC feature extractor\n",
    "    F = FeatureExtractor(input_dim=d, domain_dim=num_source_domains+1, feature_dim=2)\n",
    "    F = train_feature_extractor(F, X_train, Y_train, domain_ids_train, epochs=20, lr=1e-3)\n",
    "    # Train linear regressor on constructed space\n",
    "    R = LinearRegressor(feature_dim=2)\n",
    "    R = train_regressor(F, R, X_train, Y_train, domain_ids_train, epochs=10, lr=1e-3)\n",
    "    # Evaluate on test set (target domain)\n",
    "    X_test = dat_test[:, 1:]\n",
    "    Y_test = dat_test[:, 0]\n",
    "    domain_ids_test = np.full(len(X_test), num_source_domains, dtype=int)\n",
    "    X_test_torch = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    domain_ids_test_torch = torch.tensor(domain_ids_test, dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        features_test = F(X_test_torch, domain_ids_test_torch)\n",
    "        preds = R(features_test).cpu().numpy()\n",
    "    mse = np.mean((preds - Y_test)**2)\n",
    "    darc_mse[b] = mse\n",
    "    print(f\"Fold {b+1}, Test MSE: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "bentoCellName": {
     "name": "Calculate DARC MSE",
     "origin": "ai"
    },
    "customInput": null,
    "executionStartTime": 1767136069837,
    "executionStopTime": 1767136070110,
    "language": "python",
    "originalKey": "1227e54b-dfa8-460f-bdbb-b8a70cb98dad",
    "output": {
     "id": "1629010928416622",
     "output_revision_id": "1584167932617118"
    },
    "outputsInitialized": true,
    "requestMsgId": "48da5b3a-b080-4b36-bcf3-1413f33bcaca",
    "serverExecutionDuration": 4.6636660117656,
    "showInput": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92067408, 2.19517393, 1.73162177])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "darc_mse"
   ]
  }
 ],
 "metadata": {
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "last_base_url": "https://bento.edge.x2p.facebook.net/",
  "last_kernel_id": "9f7f9ce3-4848-47e7-a99f-206063aad4b6",
  "last_msg_id": "e69a17fd-6a39cc934e1c21b4d1a9e144_114",
  "last_server_session_id": "ce56a8dd-2fce-4cf4-bb86-2435bb2eebf6"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
